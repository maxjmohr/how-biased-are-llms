\section{Introduction}
\par In recent years, large language models (LLMs) have emerged as transformative tools for natural language processing, redefining the boundaries of what artificial intelligence can achieve. State-of-the-art models are capable of generating text, understanding and solving complex problems and reasoning over abstract concepts \parencite{naveed2023comprehensive, zhao2023survey}. Further, the models have evolved to also understand visual and auditory inputs, enabling them to perform a wide range of tasks across different modalities. As a result, LLMs are increasingly applied in various domains such as content creation, customer support, code generation and many more \parencite{hadi2024large}.

\par Despite their extensive capabilities, the growing use of LLMs has raised concerns about the potential biases and ethical implications in their decision-making and reasoning processes. Research has shown that LLMs do not only capture the statistical and linguistic patterns in the training data but also learn general knowledge and human patterns. This has led to research regarding the existence of cognitive biases as systematic deviations from rationality in the decision-making of language models \parencite{tversky1974judgment, schramowski2022large}.

\par Understanding these biases is critical for ensuring the ethical and fair usage of these models in practice \parencite{echterhoff2024cognitive}. In some application areas such as medical diagnosis, biases can have severe consequences and, hence, models with fewer biases are preferred \parencite{haltaufderheide2024ethics}. In other areas such as market research, human-like behavior and decision patterns in the models can be desirable \parencite{talboy2023challenging}. Researchers have analyzed the presence of cognitive and other biases either qualitatively or quantitatively \parencite{dominguez2023questioning,echterhoff2024cognitive,talboy2023challenging} but not in a standardized and comparable manner.

\par To address this research gap, we focus on the following research questions:
\begin{enumerate}[itemsep=0pt, parsep=0pt, topsep=0pt]
    \item Do large language models exhibit cognitive biases in their decision-making?
    \item What are the effects of distinct prompt adjustments on the bias detections?
    \item Are specific models and model features more prone to biases?
\end{enumerate}

\par To answer these questions, we conduct a series of experiments with different types of biases and model configurations. We process the responses into a standardized target variable and quantitatively analyze the detections. With our target variable, we investigate the impact and explainability of different prompt scenarios as well as model features on the biases detected.

\par We structure the thesis as follows: In Chapter \ref{chapter:theoreticalbackground}, we provide an overview of the existing research on cognitive biases and their presence in LLMs. We then introduce the selected biases and models as well as describe the study design and analysis of bias detections in Chapter \ref{chapter:methodology}. In Chapter \ref{chapter:results}, we present the results of our experiments and discuss the implications of the biases detected. Lastly, we conclude our findings and provide an outlook on future research in Chapter \ref{chapter:discussionoutlook}.