\section{Existing literature}
\label{chapter:theoreticalbackground}

\subsection{Past research on human behavioral effects}
\par The study of human behavior has been a central focus of research as it provides insights into how individuals make decisions and interact with their environment. In trying to explain behavioral patterns, researchers have identified various cognitive biases that influence human decision-making. Generally, cognitive biases refer to systematic patterns of deviation from rational judgment or objective standards \parencite{tversky1974judgment}. While traditional decision-making theories, such as classical economics, assume that individuals make decisions based on logic and reason, research over the past several decades has demonstrated that human decision-making is deeply influenced by psychological factors, leading to predictable errors in judgment. In fact, it has been estimated that 70\% of all decisions by humans are affected by cognitive biases \parencite{juarez2018analyzing}. \textcite{kahneman2017thinking} argues that these biases are the result of an attempt to simplify complex information, but often at the expense of accuracy and rationality.

\par One influential theory to challenge the notion of human rationality is \textit{Bounded Rationality}, introduced by Herbert \textcite{simon1955behavioral, simon1990bounded}. Simon proposed that humans generally strive to make rational decisions. However, they can be limited for various reasons such as limited available information, cognitive limitations or time pressure. Instead of optimizing the decision, humans often weigh the costs and benefits of the decision and select the first option that meets a certain threshold of acceptability. This behavior is also known as \textit{Satisficing} \parencite{simon1955behavioral, simon1990bounded}. Another significant contribution to the understanding of cognitive biases is the \textit{Prospect Theory} developed by \textcite{kahneman1979prospect}. This theory challenges the traditional economic view of human decision-making by stating that individuals do not always act in ways that maximize expected utility (i.e. they are not always rational). Instead, humans tend to overweight potential losses compared to equivalent gains, a phenomenon known as \textit{Loss Aversion}. They also evaluate outcomes relative to a reference point (often the status quo) rather than considering absolute values. The implications of \textit{Prospect Theory} are profound, as it suggests that human behavior deviates from the predictions of classical economic models in a systematic manner, especially under conditions of risk and uncertainty.

\par Over time, research has led to the identification of much more biases (over 180), often through the lens of neuroscience and behavioral economics \parencite{azzopardi2021cognitive}. As the field has advanced, researchers have developed frameworks to categorize when cognitive biases occur. \textcite{arnott1998taxonomy}, for example, proposed a taxonomy of cognitive biases based on different abstraction levels and situations. \textcite{dimara2018task} present a taxonomy for cognitive biases (for information visualization) based on the tasks they are associated with, such as decisions, estimations or hypothesis assessment. \textcite{kahneman2017thinking} researched the underlying reasons for the occurrence of these biases and introduced the concept that humans operate and decide using two distinct modes of thinking: an automatic, fast, and intuitive system (\textit{System 1}) and a conscious, slow, and analytical system (\textit{System 2}). He argues that \textit{System 1} is responsible for most of the cognitive biases, as it operates automatically and effortlessly, while \textit{System 2} is responsible for more deliberate and controlled thinking.

\par Despite having a negative connotation, cognitive biases are not necessarily harmful. \textcite{gigerenzer2007gut} argues that cognitive biases can be beneficial in certain situations, as they allow individuals to make quick decisions in complex environments. For example, the \textit{Availability Heuristic} allows individuals to make decisions based on readily available information, which can be useful in situations where quick decisions are required. Recently, researchers have also started to investigate and develop interventions that help individuals recognize and counteract biases, such as nudges \parencite{thaler2008nudge}. Still, cognitive biases must also be recognized as systematic deviations which can lead to significant error in judgement. Thus, it is crucial to understand the biases and the context to assess whether they are beneficial or harmful. In summary, cognitive biases represent a systematic departure from rational decision-making, and understanding these biases is crucial for explaining human behavior.


\subsection{Leveraging large language models to simulate human behavior}
\par The development of natural language processing (NLP) is rapid and ongoing. Progressing from statistical models with basic next token predictions based on the Markov Theorem (previous token as the best predictor) over neural language modelling with recurrent neural networks (RNNs) to pre-trained language models (PLMs) and most recently large language models, the field has seen significant advancements in the past decade. The self-supervised learning approach on large corpora of text data as well as the Transformer architecture have been the key drivers of success of PLMs. The Transformer architecture, introduced by \textcite{vaswani2017attention}, allows for parallelization and efficient training of large models. Since its introduction, the architecture has been adapted and improved in various ways and model parameters have been scaled up to billions of parameters to form large language models. These models have achieved state-of-the-art performance on a wide range of NLP tasks, such as text classification, question answering, and language translation \parencite{naveed2023comprehensive, zhao2023survey}.

\par The success of large language models has led to their widespread adoption in various areas. However, the models' increasing usage has raised concerns about the potential biases they may exhibit. While training a LLM, there are different ways in which biases can be introduced. Firstly, the data used to train the model may contain biases \parencite{bender2021dangers, naveed2023comprehensive, zhao2023survey}. Training data often includes text, code or other forms of human-generated data, from official sources like books, articles or websites to user-generated content like social media posts or comments. These data sources can contain wanted and undesired biases, which is thus a cause for the importance of data preprocessing and cleaning prior to training. For example, \textcite{gebru2021datasheets} point out that machine learning models pick up biases present in the training data and therefore suggest accompanying each dataset with a "datasheet" that documents the data collection and recommended uses. Transparency of LLM providers can help to understand the data sources and the preprocessing steps taken \parencite{naveed2023comprehensive, zhao2023survey}.

\par Secondly, developers desire that their models possess human values and ethical standards, i.e. human alignment. The collection of human feedback is a common approach to ensure this alignment. Reinforcement Learning with Human Feedback (RLHF) is a technique that allows developer to train a pre-trained language model using a reward model that is based on human feedback. Before entering RLHF, many developers fine-tune their pre-trained models with supervised learning for specific tasks and correct output formatting (e.g. instruction-tuning). With the pre-trained LLM, developers then generate multiple responses per prompt which are then rated by human annotators. The ratings are used to train a reward model than predicts the human preferences. Using both the language and reward model, the LLM is then fine-tuned with RLHF. While the explained approach of rating an entire response model is known as \textit{outcome-supervised} RLHF, another approach is \textit{process-supervised} RLHF, where the human annotators rate the model's intermediate steps (e.g. sentence, word or reasoning) to provide more detailed feedback. Other methods focus on aligning the data collection from the beginning, also leveraging a reward model to select high quality responses for future training and fine-tuning \parencite{ouyang2022training, zhao2023survey}. Many more approaches exist to ensure human alignment in language models which emphasizes in how many ways biases can be introduced into LLMs.

\par The possible inclusion of biases in language models has led to growing research contributions to the field of cognitive biases and human behavior in LLMs. \textcite{talboy2023challenging} qualitatively assess LLM responses and indicate the existence of several cognitive biases in a small selection of language models and present best practices in the usage of LLMs. \textcite{dominguez2023questioning} investigate biases occurring in human surveys (e.g. ordering and labeling bias) and compare the response distributions of humans and LLMs, thus using a quantitative approach. They further demonstrate that through prompt adjustments, the model responses trend towards uniform random distributions. There is also research promoting the idea to consciously include cognitive biases into AI algorithms to enhance the efficiency of the models decision-making and faster learning from less data \parencite{hagendorff2024we, taniguchi2018machine}. As previously stated, biased decision-making in LLMs can both be seen as beneficial and harmful, depending on the context, e.g. assisting in law enforcement or healthcare vs. human resources or market researches \parencite{zhao2023survey}. The interest in applying LLMs in market research is growing as research shows that for certain scenarios, the models already decide realistically and comparably to humans; for scenarios where the responses still lack human-like intuition, studies suggest that the models can be further aligned with different prompt structuring, fine-tuning and new model generations \parencite{brand2023using, qiu2023much}.