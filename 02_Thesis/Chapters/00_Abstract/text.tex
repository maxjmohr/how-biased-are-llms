\section*{Abstract}
\par Large language models (LLMs) have become a cornerstone in natural language processing with numerous applications in research and industry. However, the models' ability to generate human-like text has raised concerns about the presence of biased decision-making processes in the models. This thesis investigates the existence of human cognitive biases in LLMs. We address the research questions of whether these models exhibit cognitive biases, how prompt differences influence bias detections and whether certain models and model features are more prone to types of biases. We develop a standardized methodology to quantitatively detect biases across 8 cognitive biases, 10 language models with 5 different model temperatures and 4 different prompt scenarios. Our results confirm that LLMs frequently exhibit biases that resemble those found in human decision-making processes. Especially the \textit{anchoring bias}, \textit{endowment effect}, \textit{framing effect} and \textit{loss aversion} were detected consistently across multiple models. We find significant effects of the model size, temperature and certain prompt scenarios to help explain the biases' presence in LLMs.

\vfill
\begin{flushbottom}
\par All source code and data including model responses as well as bias detections used in this thesis is available in the 
\href{https://github.com/maxjmohr/MSc_04_Master_Thesis}{\textit{GitHub repository}}.
\end{flushbottom}
