\section*{Abstract}
\par Large language models (LLMs) have become a cornerstone in natural language processing and various applications in research and industry. However, the models' ability to generate human-like text has raised concerns about the presence of biased decision-making processes in the models. This thesis investigates the existence of human cognitive biases in LLMs. We address the research questions of whether these models exhibit cognitive biases, how prompts and variable values influence bias detections and whether certain models and model features are more prone to biases. We develop a standardized methodology to quantitatively detect biases across 8 biases, 10 language models with 5 different temperatures each and 4 different prompt scenarios. Our results confirm that LLMs frequently exhibit biases that resemble those found in human decision-making processes. Especially the \textit{anchoring bias}, \textit{endowment effect}, \textit{framing effect} and \textit{loss aversion} were detected consistently across multiple models. We find significant effects of the model size, temperature and some prompt scenarios to help explain the biases' presence in LLMs.

\vfill
\begin{flushbottom}
\par All source code and data including model responses as well as bias detections used in this thesis is available in the 
\href{https://github.com/maxjmohr/MSc_04_Master_Thesis}{\textit{GitHub repository}}.
\end{flushbottom}
