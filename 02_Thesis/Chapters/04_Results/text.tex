\section{Results}
\label{chapter:results}

\subsection{Analysis of detected biases per bias and model}
\subsubsection{Bias detections}
\par The distribution plot in Figure \ref{fig:detections-distribution} portrays a high portion of bias detections around 0. In fact, the amount of detections where we had no effect between the means of the two groups (\textit{bias detected = 0}) is 59.2 \%. This means that in roughly six out of ten cases, the models did not respond differently at all (in their means) to the two prompts. Further, we find that 80.4 \% of all bias detections are within the expectations and interpretation guidelines by \textcite{cohen1988statistical} (between -1 and 1). For our \textit{bias detected (capped)} metric, we capture 74.2 \% of the original detections and cap the outliers for interpretability purposes  as all effects above 0.8 can be considered as large effect sizes. The distribution displays a slight tendency above 0, indicating more biased than unbiased responses (32.2 \% > 0 and 8.6 \% < 0).

\begin{figure}[htbp]
    \centering
    \includesvg[width=0.9\textwidth]{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Overview/bias_detections_distribution.svg}
    \caption[Distribution plot of bias detections]{\centering \textit{Distribution of bias detections (uncapped). Plot depicts distributions of bias detections between -1.7 and 1.7 which represents 87.4 \% of all detections (for visualization purposes).}}
    \label{fig:detections-distribution}
\end{figure}

\par Aggregating across all biases and models, we find a total of 28 bias detections (\textit{bias detected (capped)} \geq\, 0.5). We display the aggregations as a heatmap in figure \ref{fig:detections-heatmap} and can thus examine the bias detections across biases and models. We average across the unmodified bias detections and afterwards limit the aggregates between 0 and 1 to allow for a better interpretation. This leads to a high density of bias detections around 0 and 1 but. However, as the scaling in between the minimum and maximum stays untouched, we are still able to apply the interpretation guidelines for small, medium and large effects by \textcite{cohen1988statistical}.

\begin{figure}[htbp]
    \centering
    \includesvg{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Overview/heatmap_detections.svg}
    \caption[Heatmap of bias detections grouped by biases and models]{\centering \textit{Detected biases grouped by biases and models. 0 signals non-existent and 1 fully-existent bias. Detailed calculation and aggregation of the target variable described in chapters \ref{methodologies:biasdetector} and \ref{methodologies:analysisbiasmodels}, respectively.}}
    \label{fig:detections-heatmap}
\end{figure}

\par The heatmap depicts a high number of bias detections for the \textit{anchoring bias}, the \textit{endowment effect}, the \textit{framing effect} and \textit{loss aversion}. Especially the \textit{anchoring bias} is detected with a large effect in all models. We examine that all models are heavily influenced in their estimations when being exposed to a reference point (anchor). The \textit{framing effect} is also detected in most models, some with a larger effect than others. Whether the model associates a monetary loss directly with another purchase seems to lead to different outputs. Generally, models with more parameters seem to be more biased towards this, with the small phi3.5 model being an exception. When detected, the \textit{endowment effect} also has a large impact on the models' responses. All models except for llama3.1 and phi3:medium are biased towards valuing their own possession higher than an offered item. \textit{Loss aversion}, if detected, shows strong to very strong effects in the models or none at all. Especially the model families by Anthropic and OpenAI show strong tendencies to avoiding losses and therefore taking more risks. Smaller models show less bias towards \textit{loss aversion}. Interestingly, llama3.1:70b, which is one of the larger models we analyze and larger than its 8 billion parameter twin, shows less bias than the smaller model.

\par Some biases were not detected in any of the models. Both the \textit{gambler's fallacy} and \textit{category size bias} are biases that examine the statistical understanding of independent subjects and events. The large language models seem to not show any bias towards these fallacies. The \textit{sunk cost fallacy} is also not detected in any of the models. Humans generally prefer the option (two simultaneous ski trips) with the larger sunk costs, independent of their actual emotional preference. Interestingly, we find that the models not only do not exhibit this bias, but prefer the emotionally preferred option with lower sunk costs. This could also be caused due missing relationship and reference towards the monetary value of the sunk costs.

The \textit{transaction utility bias} is an extreme case amongst our biases as it is not detected in any models except for gpt-4o and claude-3.5-sonnet. The experiment aimed to investigate if participants would shift your purchasing decision based on the transaction utility of the product (difference between prices). While human participants altered their decision based on the relative amount of money saved to the purchasing prices, most of the models understood that both scenarios had the same absolute price difference. As the experiment was also linked to a 20-minute drive to purchase the cheaper product, the models might not have understood the physical implication of this part compared to humans. However, gpt-4o and claude-3.5-sonnet did seem to understand the influences in the experiment and showed a strong bias effect.

\par From a model perspective, \textit{GPT-4o} and \textit{Claude-3.5-Sonnet} are the most biased models among our bias and model combinations. While the \textit{anchoring bias} is not a strongly detected as in other models, \textit{GPT-4o} either shows strong bias detections or no bias detections at all. The smaller twin \textit{GPT-4o-Mini} shows similar but less pronounced behavior except for the anchoring bias. We find similar patterns for the \textit{Claude}, \textit{Gemma} and \textit{Llama} models, where the smaller model often shows less biased behavior than the larger model. The \textit{Phi} models show a different pattern, where the larger model is less biased than the smaller model. \textit{Phi3:Medium} is in fact the least biased model of our selections. This could be due to the later training of \textit{Phi3.5} and thus newer data with model improvements or the much larger context length.


\subsubsection{Homogeneities of bias detections}
\par We also test the detected biases for homogeneity. The focus lies on whether a bias was detected and not on the exact effect size. Therefore, in contrast to the aggregated bias detections, we transform the original bias detections before calculating the homogeneities, then set limits at 0 and 1 and display them in a heatmap (see Figure \ref{fig:homogeneity-heatmap}). Applying the 75\% rule by Hunter and Schmidt (REF), we particularly find homogeneity for non-bias detections.

\begin{figure}[htbp]
    \centering
    \includesvg{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Homogeneity/homogeneity_heatmap.svg}
    \caption[Heatmap of homogeneities grouped by biases and models]{\centering \textit{Homogeneities of capped effect sizes (between 0 and 1) grouped by biases and models. Each homogeneity is calculated as the ratio of variance due to sampling error and observed variance (detailed in chapter \ref{methodologies:analysisbiasmodels}).}}
    \label{fig:homogeneity-heatmap}
\end{figure}

\par As we expected, slight and strong bias detections are quite heterogeneous across the bias-model combinations. This indicates that the effect sizes (even though capped) are not consistent across the combinations. The averaging of the effect sizes of all scenarios and model temperatures can be one reason for the heterogeneity. The small but perhaps influential adaptions to the model prompts and their response behavior could lead to slightly different model responses and thus effect sizes. Further, re-initializing the models between each experiment could also lead to inconsistent responses. Though, there are some exceptions: for the \textit{anchoring bias} in \textit{Phi3:Medium}, we find a very homogenous, high bias detection. Other than that for bias detections \geq\, 0.3, we only find slight homogeneity of \geq\, 0.3 for the \textit{anchoring bias} in \textit{Llama3.1:70b} and the \textit{framing effect} in \textit{Claude-3.5-Sonnet}. We also detect a few cases of low bias detections with low homogeneity; all homogeneity anomalies are listed in Table \ref{tab:homogeneity_anomalies}. A low homogeneity even though we did not detect a bias could indicate that in some scenarios or with some model temperatures, a bias is more present than in others. Therefore, we need to further analyze the bias detections to understand the model behaviors in more detail.

\subsubsection{Regression analysis of biases and models}
\par The regressions on the uncapped and capped bias detections (results in Appendix \ref{appendix:regr_allexper}) show that the biasness of the models significantly depend on the explored bias ($p < 0.05$). This is consistent with the hypothesis that the detections seem to especially be dependent of the specific bias itself, as suggested by the heatmap in Figure \ref{fig:detections-heatmap}. The regression on \textit{bias detected} has a very low explained variance $R^2$  of 7.2 \% which likely reflects the influence of outliers and variance in the exact magnitude of the bias detections, aligning with the heterogeneity analysis of the bias detections. The regression on \textit{bias detected (capped)} has a higher explained variance of 36.7 \%, suggesting that capping bias detections helps reduce the influence of outliers. This allows us to better capture meaningful effects between the experiment variables and bias detections and rather estimate whether a model is biased at all instead of its exact bias magnitude. For the latter regression, most models do not significantly influence bias detections except for the two largest models, \textit{GPT-4o} and \textit{Claude-3.5-Sonnet}. Both have significant positive effects on the bias detections, indicating that these models are more prone to biases than the other models. This is consistent with our expectation that models with more parameters can better inherit and learn biases from the training data.


\subsection{Impact of scenarios}
\label{results:scenarioimpact}
\par From our regression results in Appendix \ref{appendix:regr_allexper}, we find that the exclusion of the persona prompt leads to significantly lower bias detections across the unmodified effects (-2.16). This aligns with our expectation that the persona leads to more human-like and thus more biased responses. Both the scenarios with odd and extremely large numbers had no significant impacts on the magnitude of the effects. While non-significant, the results suggest opposite tendencies as the odd values rather led to a higher biasness and the extremely large values to a lower biasness.

\begin{figure}[htbp]
    \centering
    \includesvg[width=0.7\textwidth]{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Scenarios/scenario_detections.svg}
    \caption[Distribution plots of bias detections by scenarios]{\centering \textit{Bias detections (uncapped) grouped by scenarios. Plot depicts distributions of bias detections between -5 and 5 (for visualization purposes).}}
    \label{fig:scenario-detections}
\end{figure}

\par A glimpse at the distributions in Figure \ref{fig:scenario-detections} and the regression results of the scenarios on \textit{bias detected (capped)} extends our analysis. The distribution plots do not display clear differences except a slight decrease of minimal bias detections in the scenario with extremely large values. This aligns with the regression results where this scenario has the only significant effect with a slightly negative effect (-0.07) on the bias detections compared to the base scenario. The scenario with similar but odd numerical values did not show any significant impact in both regressions. Further, the regressions exclusively of the scenarios on the bias detections (Appendix \ref{appendix:regr_scenarios}) indicate that the explained variance of the scenarios is very low ($R^2 \leq 1 \%$). The low $R^2$ indicates that scenarios alone do not sufficiently explain the variability in bias detections, suggesting other factors play a more prominent role. While some scenarios do show slight significant impacts and tendencies (missing persona and large numbers lead to lower biasness in the models), the overall influence of the scenarios on the bias detections appears to be minimal.


\subsection{Model feature analysis}
\label{results:modelanalysis}
\par To further investigate the language models and their bias detections, we regress selected model features on both \textit{bias detected} and \textit{bias detected (capped)}. The results of the regressions are displayed in Appendix \ref{appendix:regr_modelfeats}. As for the scenario regressions in \ref{results:scenarioimpact}, the regression models have little explained variance, the latter regression without the outliers and solely focusing on the existence of the bias effects fitting the data slightly better with an $R^2 = 4.1 \%$. For this regression, we find some significant effects. Most notably, a higher temperature significantly increases the bias detection in a model (by \textasciitilde{0.07}). This aligns with our expectations as a higher model temperature generally leads to more creative and thus more human-like, biased responses, also probably differing more between the two prompts in each experiment. Further, larger models with more parameters show slight gains (0.0015) for detecting biases. This overlaps with our previous analysis where the larger models \textit{GPT-4o} and \textit{Claude-3.5-Sonnet} showed higher bias detections than other models.

\par We also find a significant though minor effect for the knowledge cutoff of the models. An earlier training data cutoff leads to a slightly higher \textit{bias detected (capped)} (by 0.0004). This could be due to enhanced data curation and preprocessing as well as new training methods leading to less model intake of biases. However, the effect is minor and should not be overinterpreted. Both model evaluation metrics \textit{MMLU} and the \textit{Chatbot Arena Score} do not significantly explain the existence of a bias. Though the \textit{Chatbot Arena Score} has a significant impact on the uncapped \textit{bias detected}, the effect is small (0.02 while the intercept coefficient is roughly -24.5). Still, this points out that the human-likeliness scores in the Chatbot Arena could be a minor indicator for models with more human-like behavior and biases. In neither regression, the release and last-updated dates had significant effects on the bias detections. This suggests that amongst our selected models, the model's age and the time since the last update do not significantly influence the bias detections.
