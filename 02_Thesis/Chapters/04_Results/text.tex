\section{Results}
\subsection{Analysis of detected biases}
\par Aggregating across all biases and models, we find a total of 28 bias detections (\textit{bias detected} \geq\, 0.5). Displaying the aggregations as a heatmap in figure \ref{fig:detections-heatmap}, we can examine the distribution of bias detections across biases and models. As we aggregate the bias detections with the true effect sizes and afterwards cap off the aggregates, we find a large portion of the aggregate effect sizes either indicating no bias detection (0) or a full bias detection (1). However, the scaling in between the minimum and maximum stays untouched, we are still able to apply the interpretation guidelines by \textcite{cohen1988statistical}.

\begin{figure}[htbp]
    \centering
    \includesvg{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Overview/heatmap_detections.svg}
    \caption[Heatmap of bias detections grouped by biases and models]{\centering \textit{Detected biases grouped by biases and models. 0 signals non-existent and 1 fully-existent bias. Detailed calculation and aggregation of the target variable described in chapters \ref{methodologies:biasdetector} and \ref{methodologies:analysisbiasmodels}, respectively.}}
    \label{fig:detections-heatmap}
\end{figure}

\par The heatmap depicts a high number of bias detections for the \textit{anchoring bias}, the \textit{endowment effect} and the \textit{framing effect}. Especially the \textit{anchoring bias} is detected with large impact in all models. This tells us that the models are heavily influenced in their estimations when being exposed to a reference point (anchor). The \textit{framing effect} is also detected in the models, some with a larger effect than others. Whether the model associates a monetary loss directly with another purchase seems to lead to different outputs. Generally, models with more parameters seem to be more biased towards this, with the small phi3.5 model being an exception. If detected, the \textit{endowment effect} also has a large impact on the models' responses. All models except for llama3.1 and phi3:medium are biased towards valuing their own possession higher than an offered item.

\par Some biases were not detected in any of the models. Both the \textit{gambler's fallacy} and \textit{category size bias} are biases that examine the statistical understanding of independencies. The large language models seem to not show any bias towards these fallacies. The \textit{sunk cost fallacy} is also not detected in any of the models. Humans generally prefer the option (two simultaneous ski trips) with the larger sunk costs, independent of their actual emotional preference. Interestingly, we find that the models not only do not exhibit this bias, but prefer the emotionally preferred option with lower sunk costs. This could also be caused due missing relationship and reference towards the monetary value of the sunk costs. \textit{Loss aversion}, if detected, shows strong to very strong effects in the models or none at all. Especially the model families by Anthropic and OpenAi show strong tendencies to avoiding losses and therefore taking more risks. Smaller models show less bias towards \textit{loss aversion}. Interestingly, llama3.1:70b, which is one of the larger models we analyze and larger than the 8 billion parameter twin, shows less bias than the smaller model.

The \textit{transaction utility bias} is an extreme case amongst our biases as it is never detected in the models except for gpt-4o and claude-3.5-sonnet. The experiment aimed to investigate if participants would shift your purchasing decision based on the transaction utility of the product (difference between prices). While human participants altered their decision based on the relative amount of money saved to the purchasing prices, most of the models understood that both scenarios had the same absolute price difference. As the experiment was also linked to a 20-minute drive to purchase the cheaper product, the models might not have understood this part of the impact on the decision-making process. However, gpt-4o and claude-3.5-sonnet did seem to understand the influences in the experiment and showed an unequivocal bias towards the \textit{transaction utility bias}.

\par Generally, gpt-4o and claude-3.5-sonnet are the most biased models amongst our bias and model combinations. While the anchoring bias is not a strongly detected as in other models, gpt-4o either shows strong bias detections or no bias detections at all. The smaller twin gpt-4o-mini shows similar but less pronounced behavior except for the anchoring bias. We find similar patterns for the claude, gemma and llama models, where the smaller model often shows less biased behavior than the larger model. The phi models show a different pattern, where the larger model is less biased than the smaller model. Phi3:medium is in fact the least biased model of our selections. This could be due to the later training of phi3.5 and thus newer data with model improvements. We analyze the explainability of bias detections through model features in chapter \ref{results:modelanalysis}.


\par We also test the detected biases for homogeneity. The focus lies on whether a bias was detected and not on the exact effect size. Therefore, in contrast to the aggregated bias detections, we transform the original bias detections before the homogeneity calculation and set limits at 0 and 1. Again grouping the detections by biases and models, we display the homogeneities (portion of sample variance against observed variance) as a heatmap (see figure \ref{fig:homogeneity-heatmap}). Applying the 75\% rule by Hunter and Schmidt (REF), we particularly find homogeneity for non-bias detections.

\begin{figure}[htbp]
    \centering
    \includesvg{/Users/mAx/Documents/Master/04/Master_Thesis/02_Thesis/Chapters/04_Results/Homogeneity/homogeneity_heatmap.svg}
    \caption[Heatmap of homogeneities grouped by biases and models]{\centering \textit{Homogeneities of capped effect sizes (between 0 and 1) grouped by biases and models. Each homogeneity is calculated as the ratio of variance due to sampling error and observed variance (detailed in chapter \ref{methodologies:analysisbiasmodels}).}}
    \label{fig:homogeneity-heatmap}
\end{figure}

\par As we expected, slight and strong bias detections are quite heterogeneous across the bias-model combinations. This indicates that the effect sizes (even though capped) are not consistent across the combinations. The averaging of the effect sizes of all scenarios and model temperatures can be one reason for the heterogeneity. The small but perhaps influential adaptions to the model prompts and their response behavior could lead to slightly different model responses and thus effect sizes. Further, re-initializing the models between each experiment could also have an impact on the model responses. There are some exceptions amongst this impression: for the anchoring bias in phi3:medium, we find a very homogenous, high bias detection. This indicates that the effect sizes are very consistent across the scenarios and model temperatures. Other than that for bias detections \geq\, 0.5, we only find slight homogeneity of \geq\, 0.3 for anchoring in llama3.1:70b and the framing effect in claude-3.5.sonnet. We also find a few cases of low bias detections with low homogeneity: the category size bias in llama3.1, gambler's fallacy in gemma2 and the framing effect in gemma2:27b. A low homogeneity even though we did not detect a bias could indicate that in some scenarios or with some model temperatures, a bias is more present than in others. Therefore, we need to further analyze the bias detections to understand the model behaviors in more detail.

ANHANG DIE TABELLE MIT DEN AUFFÄLLIGEN HOMOGENITÄTEN

\subsection{Scenario impact}
Xxx

\subsection{Model analysis}
\label{results:modelanalysis}
Xxx

\subsection{Interactions}
Xxx
