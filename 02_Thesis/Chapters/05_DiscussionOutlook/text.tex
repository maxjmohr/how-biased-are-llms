\section{Conclusion and discussion}
\label{chapter:discussionoutlook}

\par In this thesis, we have explored the existence of human cognitive biases in LLMs, addressing the research questions of whether these models exhibit the biases, how persona prompting and odd and extremely large values influence bias detections and whether certain models and model features are more prone to biases. Through a comprehensive series of experiments, we have provided quantitative evidence that cognitive biases are not only present in LLMs but also vary across biases, models, their temperatures as well as prompt scenarios. We present a methodology to process the model responses into a standardized metric across all types of biases, allowing for a direct comparison of the biases' presence and magnitude. This should enable future research to build upon our findings and further investigate the biases in the language models.

\par Our results confirm that LLMs frequently exhibit biases that resemble those found in human decision-making processes. Especially the \textit{anchoring bias}, \textit{endowment effect}, \textit{framing effect} and \textit{loss aversion} were detected consistently across multiple models and scenarios. Contrary, some biases such as the \textit{category size bias}, \textit{gambler's fallacy} and \textit{sunk cost fallacy} showed no bias detections in our experiments. We thus conclude that it is pivotal to be aware of which biases are present in the LLMs and which biases a developer or researcher seeks to invoke or mitigate.

\par With our prompt adjustments, we have explored possibilities which can influence the biases detected in the models. We examine that extremely large values in the experiments lead to less biased behavior in the models. The exclusion of a persona prompt also shows shifts towards less biased behavior. Declaring the model to act humanlike and using less realistic values in the prompts which are less likely to be included in the training data are both factors that lead to more biased behavior in the models and vice versa.

\par We find that larger language models such as \textit{GPT-4o} and \textit{Claude-3.5-Sonnet} are significantly more prone to biased behavior, though the effect is small. The model temperature also has a significant effect, with higher temperatures leading to more biased responses. While the knowledge cutoff has a slight significant effect, the release and lastly-updated dates as well as model metrics show no significant impact on the bias detections. We can thus conclude that careful model selection and hyperparameter setting can help to mitigate biases in LLMs. For market research and gravitating towards human-like, biased behavior, larger models with higher temperatures which were trained on recent data seem to be more suitable than others.

\par Despite the promising results, our study has limitations. Primarily, our regression results lack explained variance, suggesting that our experiments do not fully capture the complexity of cognitive biases in LLMs yet. This makes it difficult to draw precise conclusions and best-practice advice. Additionally, our bias detection is limited to a small set of biases, and we do not consider interactions between the biases. Similarly, the model selection is limited to a relatively small set of LLMs, some of which have been updated recently. Future research should expand the set of biases and models to provide an even more comprehensive understanding and generalization of cognitive biases in LLMs. To make each bias detection more robust, the studies per bias could be expanded to include more diverse questioning targeting the bias and perhaps more reasoning-centered questions.

\par Further, other prompting techniques should be assessed to better understand their impact on biased model behavior and to form best-practice guidelines for practitioners. The rapid development of LLMs could also lead to a more extensive investigation of biases across multiple languages, i.e. multilingual, and modalities, e.g. images, videos. The latter could open up another spectrum of cognitive biases which are not necessarily existent in text-based environments. The collection of more detailed model features could provide more explainability towards why certain models are more prone to biases than others.

\par Building a framework to predict the existence and magnitude of biases in LLMs could help practitioners understand the biases in their models and potentially mitigate or invoke them. More models and model features, more biases with robust studies and more diverse prompting techniques could provide best-practice guidelines and a foundation for the ethical use of these models in the future.
