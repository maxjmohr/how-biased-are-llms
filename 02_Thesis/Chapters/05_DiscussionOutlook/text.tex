\section{Conclusion and discussion}
\label{chapter:discussionoutlook}

\par In this thesis, we set out to explore the existence of human cognitive biases in LLMs, addressing the research questions of whether these models exhibit the biases, how prompts and variable values influence bias detections and whether certain models and model features are more prone to biases. Through a comprehensive series of experiments, we have provided quantitative evidence that cognitive biases are not only present in LLMs but also vary across biases, models and their temperatures as well as prompt scenarios. We present a methodology to process the model responses into a standardized metric across all biases, allowing for a direct comparison of the biases' presence and magnitude. This should enable future research to build upon our findings and further investigate the biases in the language models.

\par Our results confirm that LLMs frequently exhibit biases that resemble those found in human decision-making processes. Especially the \textit{anchoring bias}, \textit{endowment effect}, \textit{framing effect} and \textit{loss aversion} were detected consistently across multiple models and scenarios. These findings align with prior research and suggest that LLMs learn biased patterns present in their training data and through human alignment. Contrary, some biases such as the \textit{category size bias}, \textit{gambler's fallacy} and \textit{sunk cost fallacy} showed no bias detections in our experiments. We find that the investigated cognitive bias is mostly pivotal in whether the model shows human-like biased behavior. While this human-like behavior may enhance the utility of LLMs in areas like market research, it poses ethical challenges in fields like medical diagnostics, where fairness and neutrality are paramount.

\par For our prompt adjustments, we examine that extremely large values in the experiments led to less biased behavior in the models. This suggests that the models are more likely to exhibit biases when the values are realistic and thus possibly closer to the training data. Though, the exclusion of our persona prompt does not lead to significant shifts whether a bias exists or not but rather in its magnitude. This indicates that the persona prompt is not a significant factor in bias detection but rather influences the bias level.

\par We find that larger language models such as \textit{GPT-4o} and \textit{Claude-3.5-Sonnet} are significantly more prone to biased behavior, though the effect is small. The model temperature also has a significant effect, with higher temperatures leading to more biased responses. This suggests that the models are more likely to exhibit biases when they are more creative and less deterministic. While the knowledge cutoff has a slight significant effect, the release and lastly-updated dates as well as model metrics show no significant impact on the bias detections.

\par Despite the promising results, our study has several limitations. Primarily, our regression results lack explained variance, suggesting that our experiments do not fully capture the complexity of cognitive biases in LLMs yet. Additionally, our bias detection is limited to a small set of biases, and we do not consider interactions between the biases. Similarly, the model selection is limited to a relatively small set of LLMs. Future research should expand the set of biases and models to provide an even more comprehensive understanding and generalization of cognitive biases in language models. To make each bias detection more robust, the studies per bias could be expanded to include more diverse questioning targeting the bias and perhaps more reasoning-centered questions.

\par Further, other prompting techniques should be assessed to understand their impact on biased model behavior better. For instance, the persona prompt could be further investigated by varying the persona's characteristics. Future research could explore the active biasing of a model (e.g. for market research) or debiasing techniques. The rapid development of large language models could also lead to a more extensive investigation of biases across multiple languages (multilingual) and modalities (e.g. images, videos). The latter could open up another spectrum of cognitive biases which are not necessarily existent in text-based environments.

\par Lastly, the collection of more detailed model features could provide more explainability towards why certain models are more prone to biases than others. Building a framework to can predict the existence (and magnitude) of biases in LLMs could help practitioners understand the biases in their models and potentially mitigate or invoke them. More models and model features, more biases with robust studies and more diverse prompting techniques could help to understand the biases in LLMs better and provide a foundation for the ethical use of these models in the future.