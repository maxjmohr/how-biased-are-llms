\section{Study design and methodology}
\label{chapter:methodology}

\par This section provides an overview of the methodology used in this research. We outline the selected biases and models which form the scope of the study. Further, we outline the structural design of the studies as well as the prompts and various scenarios to be tested. To evaluate the models' behaviors, we present an approach to score the bias detections and based on these results, analyze the effects of different scenarios and models.


\subsection{Bias selections}
\par When referring to the term \textit{bias} in our thesis, we refer to the cognitive biases detected in human decision-making. As described, previous research revolving around cognitive biases has shown that there are numerous biases influencing human behavior. Thus, we have to sample a concise yet ideally comprehensive sample of biases to explore more generalizable analysis results. In particular, we aim to recreate and perhaps slightly modify existing experiments. We focus on the following biases:

\setlength{\parindent}{20pt}
\par \textbf{Anchoring bias} The bias known as anchoring is often used to explain why humans tend to rely heavily on an initial piece of information ("anchor") in their decision-making afterwards. Consumers may adapt their willingness-to-pay (WTP) for a product based on the difference between an initial price (anchor) and discount price \parencite{chandrashekaran2006anchoring}. We recreate the experiment from \textcite{tversky1974judgment} where participants are first asked whether the portion of African countries in the United Nations is higher or lower than a certain number and afterwards estimate the exact percentage. Their results showed that the initial percentage had a significant effect on the exact estimation afterwards.

\par \textbf{Category size bias} Decision-making can be influenced by the way alternatives are categorized and distributed, a phenomenon known as the category size bias \parencite{isaac2014judging}. \textcite{tversky1994support} showed that participants judged the probability of dying of unnatural cases differently when natural causes were presented as one category or broken down into individual categories. To test for the category size bias, we replicate an experiment from \textcite{isaac2014judging} where participants estimate the probability of randomly selecting a ball from a lottery, itself containing balls with three different colors and varying category sizes. The results of the experiment showed that if the ball is drawn from a larger category, the probability is estimated higher even though the actual probability is equal.

\par \textbf{Endowment effect} The human tendency to value objects in their possession (endowment) higher than others is known as the endowment effect. This effect is also strongly linked to loss aversion, where people demand more to give up an item ("loss") than they would be willing to pay to acquire it ("gain") \parencite{kahneman1990experimental}. Research has also shown that the effect is independent of whether humans actually buy/sell the product or exchange similarly valued goods \parencite{knetsch1989endowment}. \textcite{kahneman1990experimental} demonstrated the effect by asking participants for their willingness-to-pay (WTP) for a mug versus their willingness-to-accept (WTA) compensation for the mug. The results showed that participants owning the mug valued it at more than double the value than the other participants. This experiment is especially interesting as the models will not have any physical possession of the mug.

\par \textbf{Framing effect} Framing is a cognitive bias where humans react significantly differently to an equivalent choice depending on how it is presented. The effect is strongly influenced by the loss aversion of humans as well as them heavily relying on their emotional state \parencite{tversky1981framing}. To detach the experiment for the framing effect as much as possible from the experiment on loss aversion, we target to simulate two scenarios that both have losses (not comparison between loss and gain framing). We recreate the experiment by \textcite{tversky1981framing} where participants were asked to buy a concert ticket after having lost either the prior purchased ticket or the same amount of money (different framing of lost monetary value). The study showed that participants were more likely to buy the ticket when they lost the money. The reason is that from the customer perspective, the lost money is decoupled from the ticket purchase and does not have an emotional connection.

\par \textbf{Gambler's fallacy} The gambler's fallacy is the mistaken belief that the probability of a certain event occurring is influenced by the frequency of prior events, despite each event being independent \parencite{bar1991perception,kovic2019gambler}. Known as an "insensitivity to sample size", humans also often tailor their decisions to a small sample size which, in their mind, represents the distribution of the larger sample. For instance, humans might quickly adapt their judgement based on the law of large numbers even though they are faced with random events \parencite{tversky1974judgment}. The shifts in probability perceptions can thus be major causes for biased (economic) decision-making. While there has been research on the mechanisms influencing the fallacy (for example the presentation of information \parencite{barron2010role}) as well as possible side effects and fallacies \parencite{kovic2019gambler}, we focus on assessing whether the models are prone to the fallacy through an experiment involving even or solely one-sided coin flips (incorporating hot hand fallacy and Monte Carlo fallacy) \parencite{leonard2015gambling}.

\par \textbf{Loss aversion} One of the most prominent and heavily researched cognitive biases is loss aversion. It describes the human tendency to prefer avoiding losses over acquiring gains of the same value  \parencite{liu2023review}. Within their research on prospect theory (decision-making biases under risk and uncertainty), \textcite{tversky1992advances} estimated that losses are twice as impactful as gains. Closely related to the framing effect, the presentation of the same base information in more risk-averse and risk-seeking scenarios can have significant effects on human decision-making \parencite{druckman2001evaluating}. This bias is particularly relevant in the context of economic decision-making and has been applied to various fields such as retail sale strategies (discount for additional spending), financial investments (sell winners, hold losers) and more \parencite{liu2023review}. We recreate the experiment from \textcite{thaler2015misbehaving} which phrases two scenarios as a financial loss and a gain to test for loss aversion. The results showed that participants were more likely to take risks in the loss scenario than in the gain scenario as loss aversion was a stronger motivator than the potential gain.

\par \textbf{Sunk cost fallacy} This effect refers to the human phenomenon of preferring an option due to a prior investment into it (sunk costs) even though a better alternative would be available \parencite{arkes1985psychology}. Due to this, even temporally distant investment decisions can have a substantial impact on the decision-making process. The sunk cost fallacy is also strongly intertwined with other cognitive biases, most notably loss aversion or commitment bias \parencite{jarmolowicz2016sunk}. We choose to examine the experiment introduced by \textcite{arkes1985psychology} where participants are asked to decide between two ski trip scenarios with different sunk costs. In their study, the participants were more likely to choose the more expensive trip (but being the less attractive trip) even though the costs were already sunk.

\par \textbf{Transaction utility theory} Transaction utility describes the perceived psychological non-monetary gains of a deal that are beyond the actual economic value. \textcite{thaler1983transaction} defined the concept as the difference between the actual price and one's reference price. A simple example is the satisfaction from buying a product itself; the additional joy of saving 20 percent is the transactional utility in the purchase. To test this theory, we will recreate an experiment where participants have the option to buy two products either directly or at a store 20 minutes away, each for a \$10 discount. The difference in the scenarios is that one product is cheap (radio) and the other is expensive (television) \parencite{thaler1983transaction}. The results showed that participants were more likely to buy the more expensive product directly without the discount even though the absolute savings were the same. For the cheaper product, participants were more willing to buy it at the other store.

\setlength{\parindent}{0pt}
\par With the selected biases, we aim to examine the cognitive decision behavior of large language models as broadly as possible. The exact replications and wordings as well as the expected outcomes of the experiments are described in Appendix \ref{appendix:biases}.


\subsection{Model selections}
\par The development and publication of new models and model architectures is ongoing and rapid. We focus on models which are widely available to the public and have seen significant use in research and industry. The models we selected are those of major technology companies and have some similarities but also differences in their architecture, size and training data. We aim to include models from different companies to ensure a broad analysis of the models' behavior. As new language models are being released every month, we set a cutoff date (August 25th, 2024) for the selection of models to ensure a consistent analysis. The models we selected are:

\setlength{\parindent}{20pt}
\par \textbf{Claude-3 \& 3.5 (Anthropic)} The Claude model family is developed by Anthropic, a company founded in 2021 by former OpenAI employees and focusing on building safe and ethical AI models. The Claude 3 models were Anthropic's first large multimodal models and are designed as transformer models. Their training included public and non-public data while respecting privacy and ethical guidelines. Besides pretraining the model on the large dataset, Anthropic use a technique called "Constitutional AI" to ensure the model's alignment with human intentions through specified rules and principles during RLHF. The model family includes three models: \textit{Haiku} focusing on lightweight tasks and speed, \textit{Sonnet} for higher complexity tasks while still being performant and \textit{Opus} for most complex tasks. All models showed significant improvements to their predecessors and  at time of release achieved state-of-the-art results in various benchmarks \parencite{anthropic2024claude}. A few months later, Anthropic released an updated version \textit{Claude 3.5 Sonnet}, outperforming the previously most performant model but operating faster and cheaper \textit{Claude 3 Opus}. With its performance and speed, \textit{Claude 3.5 Sonnet} is widely used in research and industry and therefore quite relevant for our study \parencite{anthropic2024claude2}. Due to our cutoff date, we analyze \textit{Claude 3 Haiku} and \textit{Claude 3.5 Sonnet} as the newer \textit{Haiku} model was not yet released. 

\par \textbf{Gemma2 (Google)} The Gemma model family is Google's offering of open-source language models. The latest and second generation of \textit{Gemma} models were released in late June 2024. They exhibit a decoder-only transformer architecture for text-to-text tasks and are designed similarly to Google's larger and private models (\textit{Gemini}). The focus of the Gemma2 models lies on the availability and usability to the public and thus exist in 2, 9 and 27 billion parameter sizes. The 27B model was trained from scratch on 13 trillion tokens, encompassing diverse sources like web documents, code, and mathematical texts, to ensure comprehensive language understanding. In contrast, the smaller 2B and 9B models were trained using knowledge distillation from the 27B model, enabling both to achieve competitive performance with reduced computational requirements \parencite{team2024gemma}. For our research, we examine the reasoning capabilities and existence of biases in the 9B \textit{Gemma2} and 27B \textit{Gemma2:27b} parameter versions.

\par \textbf{GPT-4o (OpenAI)} With the introduction and rash success of OpenAI's artificial intelligence chatbot \textit{ChatGPT} in 2022, the models backing the chatbot (GPTs) have a significant influence on the field of natural language processing and in communicating the latest advancements in AI to the public. The latest iteration (before our cutoff date) were the GPT-4o models (\textit{o} standing for "omni" or multi-modal) which are designed to be more human-like and capable of reasoning and understanding context. The architecture of GPT-4o is similar to its predecessors, a transformer model which includes additional layers for multimodal input, with encoders for images, videos and audio. While language processing and reasoning has improved marginally compared to GPT-4 Turbo, the tokenizer has become much more efficient and thus the model is capable of processing more data in less time and cost-efficient \parencite{achiam2023gpt, openai2024gpt4o}. We analyze GPT-4o mini, which is natively used for the free \textit{ChatGPT} service and despite its smaller parameter size outperforms the previously established model GPT-3.5 Turbo, and GPT-4o, which is the largest model and available to premium users \parencite{openai2024gpt4omini}.

\par \textbf{Llama3.1 (Meta)} At the time of our selection date, Llama3.1 was Meta's latest iteration of their open-source language model family, following the success of Llama2 and 3. Meta's offering of these foundation models has significant impact in building and finetuning personalized multilingual solutions and for any research purpose. They are among the strongest open-source models in terms of performance and capabilities. Llama 3.1 models come with much longer context length and more as well as higher quality training data than previous models while maintaining the similar transformer architecture. While we will analyze the 8B \textit{Llama 3.1} and 70B \textit{Llama 3.1:70b} parameter models, a 405B parameter version is also available and rivals the largest closed-source models. The 70B model, in particular, has demonstrated exceptional performance in benchmarks involving complex reasoning, language understanding, and code generation, making it highly effective across a wide range of tasks \parencite{dubey2024llama,meta2024llama31}.

\par \textbf{Phi3 \& 3.5 (Microsoft)} Phi by Microsoft is a family of small and large language models designed to deliver high performance across various natural language processing tasks. The models are trained on datasets including synthetic data and filtered publicly available websites. Especially the small Phi models are designed to be efficient and run on mobile devices. Despite their small parameter sizes, the models are still competitive regarding reasoning and coding tasks. There are multiple sizes and different architectures of the Phi models to account for multilingual or multimodal inputs. In particular, Phi 3.5 has a much expanded context length limit from 4 to 128 thousand tokens. We analyze the new \textit{Phi 3.5 Mini} model (roughly 4B parameters) and the slightly larger \textit{Phi 3 Medium} model (14B parameters), which was not updated prior to our selection data. With these smaller models we hope to cover not only the spectrum of large language models but also small ones \parencite{abdin2024phi}.

\setlength{\parindent}{0pt}
\par In total, we analyze five model families and ten distinct models. Each model is assigned one of 5 model temperatures in the experiments (0.2, 0.7, 1, 1.3, 1.8). Besides the GPT models of OpenAI and Claude models of Anthropic, the models are run locally. These models are downloaded as well as accessed through Ollama, a wrapper around llama.cpp which allows for efficient inference of most open-source models \parencite{gerganov2023llamacpp}. To minimize discrepancies in prompting and response extraction between the models, we use the language model framework LlamaIndex to interact with the models and build a unified experiment pipeline \parencite{liullamaindex2022}. More details on the models and their features are provided in Appendix \ref{appendix:models}. As there is less official documentation on the closed-source models and their parameters, we also rely on alternative sources to approximate some features.


\subsection{Structuring of experiments}
\par The following section will depict the structuring of the experiments. For the term \textit{experiment}, we refer to a unique combination of a bias with a scenario (detailed in section \ref{methodologies:scenarios}) run on a model with a specified temperature. An exemplary code snippet of an experiment run is appended in Appendix \ref{appendix:example_code}.

\subsubsection{Study designs}
\par We target to structure all experiments in a consistent manner to ensure comparability and a unified analysis afterwards. Most studies which we base our experiments on are structured in a way that two  questions with slightly different wording are asked to the participants. Some studies ask for value estimations while others are constructed as choice experiments (two choice options i.e. A and B). The response distributions to these questions are then compared to detect biases, identically or similarly to control and treatment group study designs. We adopt this design for each experiment where we prompt the model two questions with slight variations in wording 100 times each to form a control and treatment group \parencite{cohen1988statistical,morris2002combining}. In case we cannot correctly extract a model response, we reiterate the question as often as necessary to receive 100 valid responses. One exception is the study regarding the sunk cost fallacy where participants are asked only once to choose between two scenarios with different sunk costs \parencite{arkes1985psychology}. To align with our study design, we add a second question to act as the control where the sunk costs are identical.

\par Studies comparing two targets can be constructed as two-group design studies with independent groups or a one-group repeated measure \parencite{dunlap1996meta,goulet2018review}. Generally, to determine whether an experimental design should be classified as a two-group independent design or a single-group repeated measures design, several key assumptions must be considered. In an independent groups design, the fundamental assumption is that each group is treated as independent, meaning no participant or entity contributes data to more than one group, and there is no connection between the observations in the two groups \parencite{morris2002combining}. This design is typically used when participants or systems are randomly assigned to different conditions or when different instances are treated independently \parencite{cohen1988statistical,morris2002combining}. In contrast, a repeated measures design assumes that the same participants or system are tested under multiple conditions. This design captures within-subject variation, meaning each entity's performance under one condition is directly related to its performance under the other conditions. In this case, it is necessary to account for within-subject correlations because the same participant or system retains information or characteristics across conditions \parencite{dunlap1996meta,morris2002combining}.

\par Given our experimental design and technical implementation, each model is re-initialized between each question, meaning that it does not retain any memory or internal state between trials. Although the same model architecture is used, the fact that it is re-initialized before running the next question ensures that the responses from both groups are independent. There is no inherent correlation between the models' performances in group A and group B due to this re-initialization, which mimics the behavior of having entirely independent models for each group. Therefore, the key assumption of independence of observations is met, aligning this study with an independent two-group design. In a true repeated measures design, the models would have needed to answer both sets of questions (A and B) without resetting their state, creating a paired structure where each observation in group A could be directly compared with its counterpart in group B. Since we did not implement a paired output of both questions which would force an investigation and integration of their correlation, we argue that the appropriate framework for analysis is that of two independent groups \parencite{dunlap1996meta}.

\newpage
\subsubsection{Robust base prompt template}
\par For all experiments, we aim to create a basic prompting style similar to the questionnaires in the original studies to maintain consistency and control over the interactions across the models. Research suggests that well-structured prompts can significantly influence the quality and reliability of model outputs, especially in complex tasks like bias detection and structured information retrieval \parencite{chen2023unleashing,santu2023teler}. \textcite{santu2023teler} present a prompt taxonomy TELeR (Turn, Expression, Level of Details, Role) with suggestions on how to structure prompts for different, especially complex tasks. They generally suggest for higher complexity, the level of details and focus on structure in the prompt should increase. As we try to avoid too much additional details other than the survey itself, we orientate ourselves towards their "multi-sentence, paragraph-style" directive.

\par \textcite{sahoo2024systematic} separate prompt engineering techniques by their application domains such as reasoning tasks, reducing hallucination, generating code and many more. They present many advanced chaining and multi-step prompt engineering techniques but suggest simple prompting techniques (zero- and few-shot prompting) for new tasks without extensive model training. Especially in zero-shot settings, models rely heavily on their general knowledge gained during training. This aligns with the study designs on human participants which in most cases, are not experts and do not receive additional information besides the survey questions. Concise and clear explanations improve the response quality and in some scenarios with basic tasks, zero-shot prompts can significantly outperform guided, few-shot prompts \parencite{chen2023unleashing,ekin2023prompt}. In zero-shot scenarios and likewise in our experiments, the prompts aim to trigger a model response which it was previously not trained on \parencite{marvin2023prompt}.

\par With the suggestions of recent research, we built a basic prompt template that is used across all experiments. We do not provide additional context to the study questions besides some output formats suggestions for structured predictions (answering with a single letter or numerical value) \parencite{schmidt2024towards}. In this regard, the providing the hint that the models were only initialized to generate two tokens, improved the response quality. After some refinements and iterative testing to receive mostly structured and usable responses, we settled on the following prompt template (Table \ref{tab:prompt-template}):
\begin{table}[ht]
    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{System message}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    --------------------------
    \textit{Persona}
    --------------------------\\
    You will be asked to make choices. Please blank out that some information might be missing or that you might not be able to make a choice. I have initialized you in a way that you can only generate 2 tokens. The only valid answer is \textbf{a single letter or number}.
    \end{tcolorbox}

    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{User message}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    You are forced to choose! Answer the experiment by only giving the letter of the answer options (e.g. A, B, C, ...) or a numerical value (80, 100, 1000, ...). Do not state anything else! Do not hallucinate.\\
    --------------------------
    \textit{Study question}
    --------------------------\\
    Your output should only be a LETTER (A, B, C, ...).
    \end{tcolorbox}

    \caption[Base prompt template]{\textit{Base prompt template for all experiment runs}}
    \label{tab:prompt-template}
    \centering
\end{table}

\par Another significant factor of prompt engineering revolves around the addition of personas to the prompts. Personas are fictional characters that represent the target audience of the model and can be used to guide the model's responses towards a specific direction. Consisting of various persona variables, which can reflect demographics and other characteristics, personas have shown to influence large language models in their reasoning and responses \parencite{chen2023unleashing,hu2024quantifying,olea2024evaluating}. \textcite{hu2024quantifying} depict that personas can have small but significant impacts on the models' outputs. Their research suggests that models are able to adapt their choices to the given persona but only in a narrow range. They find that the more persona variables are directly correlated with the response variable of the model, the higher the effect of the persona. \textcite{olea2024evaluating} compare single-agent personas (either fixed or fitted to the task) and multi-agent personas and find that for simple tasks with short answers, single-agent personas are more effective. While simple personas show slight gains, especially the expert personas, which were auto-generated by a LLM for each experiment question, show significant improvements in the models' responses.

Like the studies we aim to recreate, we strive to measure a bias effect across an average, non-specific audience. For this reason, we create a simple persona with some general characteristics to fit with each experiment and aligning it to examples from \textcite{brand2023using}. This prohibits us from adding specific and correlated characteristics (relating to an expert persona) per experiment but should still provide a general guidance for the models' responses:
\begin{table}[ht]
    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{Persona}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    You are a customer with median income and average education. You are selected at random to participate in a survey. You can only choose one of the presented options or assign a value. Behave humanlike and choose instinctively. You can and are advised to make a subjective decision! There is no right or wrong, but you \textbf{have to decide}.
    \end{tcolorbox}

    \caption[Persona description]{\textit{Persona description across all experiments}}
    \label{tab:persona-description}
    \centering
\end{table}

\subsubsection{Scenarios in form of prompt adjustments}
\label{methodologies:scenarios}
\par We have set up the base prompt format and biases and models to recreate the studies. This enables us to test for the existence of cognitive biases in LLMs. While the study design can reflect the attained behavior of a model, the responses could simultaneously be influenced by the studies being part of the training data \parencite{brand2023using}. To analyze the reliance of the detected biases on familiar and learned studies, we create different scenarios across all experiments. The scenarios can range from structural prompt changes to changes of variable values in the experiments. Among finetuning of LLMs, \textcite{brand2023using} modify persona descriptions and add market insights to examine the model's product preferences. Similarly, \textcite{kojima2022large} extend the prompts by a single instruction to improve the reasoning capabilities of the models. \textcite{mizrahi2024state} observe the sensitivity of LLMs towards the exact paraphrasing of an instruction. With our scenarios, we mainly test whether the biases change with different instructions and unseen or odd and unrealistic data, as shown in Table \ref{tab:scenarios}.
\begin{table}[h!]
    \centering
    \begin{tabular}{@{}p{4cm}p{11cm}@{}}
    \toprule
    \textbf{Scenario} & \textbf{Description} \\ \midrule
    Base & Replication of the original study using base prompt format \\ \addlinespace
    Without Persona & Base scenario but exclusion of persona \\ \addlinespace
    Odd values & Base scenario but multiplication of the study variables (e.g. prices) times 9.7 \\ \addlinespace
    Large values & Base scenario but multiplication of the study variables (e.g. prices) times 55,555.5 \\ \bottomrule
    \end{tabular}
    \caption[Overview of prompt scenarios]{\centering \textit{Overview of scenarios across experiments and their descriptions on prompt adjustments}}
    \label{tab:scenarios}
\end{table}


\subsection{Response analysis}

\subsubsection{Processing responses into a bias indicator}
\label{methodologies:biasdetector}
\par With our study design of a control and test group per experiment, we aim to compare the response distributions to measure bias effects. While we are generally concerned with the existence of a bias in an experiment configuration, we especially aim to quantify the magnitude of the bias for more detailed comparisons and analysis between the experiments. With respect to identifying a bias effect, one could implement hypothesis tests for determining whether the response distributions differ significantly between the control and treatment groups. However, \textcite{sullivan2012using} assert that while P values may indicate a significant effect, the size of the effect is not extractable. P values are influenced by the effect size of differences or sample sizes and can thus be difficult to interpret. Significant P values do not necessarily stem from large effect sizes but can also e.g. from a large sample size and small (practically irrelevant) effects \parencite{borenstein2021introduction,sullivan2012using}. Further, even though methodology does exist to comparing P values and incorporating them into meta analyses, strong assumptions are needed to make the results comparable \parencite{borenstein2021introduction}.

\par As the comparability of the experiments is a key facet of our study and all experiments have similar structures and examine the same topic that is cognitive biases in large language models, we orient ourselves towards traditional meta analysis methodologies by computing a determined effect size per experiment. In our setting of two independent groups (pre- and post-test measurements), we can calculate the standardized difference of means (Cohen's d) as \parencite{borenstein2021introduction,cooper2019handbook, goulet2018review,morris2002combining, nakagawa2023quantitative} (Equation \ref{eq:cohensd}):
\begin{equation} \label{eq:cohensd}
d = \frac{\bar{X}_{post} - \bar{X}_{pre}}{SD_{pooled}}
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $\bar{X}_{pre}$, $\bar{X}_{post}$:& means of the control and treatment groups, respectively \\
    $SD_{pooled}$:& $\sqrt{\frac{(n_{pre} - 1) \cdot SD_{pre}^2 + (n_{post} - 1) \cdot SD_{post}^2}{n_{pre} + n_{post} - 2}}$ (pooled standard deviation) \\
    $SD_{pre}$, $SD_{post}$:& standard deviations of the control and treatment groups \\
    $n_{pre}$, $n_{post}$:& sample sizes of the control and treatment groups
\end{tabular} \\

\par The effect size $d$ is a standardized measure and thus allows for comparisons between different experiments and studies. Cohen's $d$ can be interpreted as the magnitude of the bias effect, with larger values indicating a stronger bias effect. The effect size can be further interpreted using the guidelines by \textcite{cohen1988statistical} where values of 0.2, 0.5 and 0.8 are considered small, medium and large effects, respectively. While Cohen's $d$ is not an unbiased estimator of the population effect size due to overestimation and a correction factor has been developed to form Hedges' $g$ \parencite{hedges1981distribution}, \textcite{goulet2018review} suggest that the correction factor is especially necessary for small sample sizes $n_{total} < 16$. Therefore, we use the uncorrected Cohen's $d$ for our analysis as all sample sizes in our experiments are $n_{total} = 200$.

\par For our experiments where we ask models to respond with numerical values, we can directly calculate the means and standard deviations of the responses to compute the effect size. To align with these experiments, we convert the models' responses in the choice experiments (i.e. A and B) to binary values, depending on which option we expect the models to choose in the treatment group. In the case of \textit{loss aversion}, we expect the control group to select the risk averse gain option ($A=0$) and the treatment group to select the risk seeking loss option ($B=1$) \parencite{thaler2015misbehaving}. We then calculate the effect size based on the binary responses.

\par By nature of Cohen's $d$ calculation (Equation \ref{eq:cohensd}), the effect size can also be negative if the control group has a higher mean than the treatment group. For most of our bias experiments, we have recreated the experiments in a way that a higher mean in the treatment group refers to the expected bias effect. We thus expect a positive effect size for the existence of a bias and interpret any negative or zero effect sizes as no bias detected. One exception to this is the experiment regarding the \textit{framing effect}. For this bias, a differentiating effect in either group hints at the existence of the framing effect. Therefore, we adjust the negative effect sizes by extracting their absolute values. The resulting metric, namely \textit{bias detected}, is the following:
\begin{equation} \label{eq:bias_detected}
    \text{\textit{bias detected}} =
    \begin{cases}
        |d| & \text{if bias = \textit{framing effect}} \\
        d & \text{else}
    \end{cases}
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $d$:& Cohen's $d$ effect size by Equation \ref{eq:cohensd} \\
\end{tabular}

\par In the subsequent analysis, we will frequently limit \textit{bias detected} to the range of 0 to 1 to simplify the interpretation. When aggregating, we may average across the original effect sizes to subsequently limit the values. This also allows us to focus more on the existence of a bias rather than the magnitude of the effect. We refer to the capped metric as:
\begin{equation} \label{eq:bias_detected_capped}
    \text{\textit{bias detected (capped)}} = \min(1, \max(0, \text{\textit{bias detected}}))
\end{equation}

\subsubsection{Analysis of effect sizes across biases and models}
\label{methodologies:analysisbiasmodels}
\par The \textit{bias detected} metric allows us to compare the bias effects across all experiments. To follow our research question of whether biases are present in large language models and how they differ between biases and models, we group all experiment detections by bias and model. \textcite{morris2002combining} suggest an average weighted by the reciprocal of the sampling variance of the effect sizes $\hat{\sigma}^2_{e_{i}}$ to aggregate effect sizes. Per experiment, the sampling variance is influenced by the sample sizes and the population effect size (unweighted average). As all our experiments have the identical sample sizes and each group has one fixed unweighted population effect size, the sampling variance is constant across all experiments of an aggregate. We can thus directly average the effect sizes to aggregate the bias detections (derivations in Appendix \ref{appendix:samplingvar_mean}). With the aggregated effect sizes, we compare the detections across biases and models to analyze the presence and magnitude of biases in the models. To simplify and illustrate the aggregated bias detections, we cap the aggregates at 0 and 1 to create \textit{bias detected (capped)} (Equation \ref{eq:bias_detected_capped}). As we solely cut off the aggregates and do not rescale the effect sizes themselves, we ensure to correctly weigh extremely large and small effect sizes in the aggregation. Further, we maintain the interpretation guidelines by \textcite{cohen1988statistical} where values of 0.2, 0.5 and 0.8 are considered small, medium and large effects, respectively.

\par Besides the bias detections themselves, we test for homogeneity of the effect sizes across the models and biases. A consistent bias detection across all experiments of a chosen subgroup would indicate a homogeneity of the effect. The homogeneity can be tested by comparing the theoretical variance due to sampling error $\hat{\sigma}^2_{e}$ with the observed variance of the effects sizes $\hat{\sigma}^2_{d}$ \parencite{borenstein2021introduction, cooper2019handbook, morris2002combining, nakagawa2023quantitative}. The derivations of both variances are detailed in Appendix \ref{appendix:samplingvar_mean}. \textcite{hunter2004methods} propose a simple comparison and suggest that if the proportion of the theoretical to the observed variance exceeds 75 \%, the effect size is regarded as homogeneous, i.e.:
\begin{equation} \label{eq:homogeneity}
    \frac{\hat{\sigma}^2_{e}}{\hat{\sigma}^2_{d}} \geq 0.75
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $\hat{\sigma}^2_{e}$:& theoretical variance due to sampling error \\
    $\hat{\sigma}^2_{d}$:& observed variance of the effect sizes \\
\end{tabular} \\

\par Our expectation is that the effect sizes are not homogeneous across the biases and models. We expect the model answers to differ enough to often create slight discrepancies in the bias detections. The different scenarios and model temperatures should further increase the variance of the effect sizes. To counter the influence of magnitude in the bias detections, we compute the homogeneities per bias and model for \textit{bias detected (capped)}. By this, we focus on whether we have homogeneous presence of biases across the experiments.

\par Lastly, we model the target variable \textit{bias detected} with our four variables \textit{bias}, \textit{model}, \textit{scenario} and \textit{temperature}. We designed all experiments as combinations of the identical variable options. This not only ensures comparability across the experiments but enables us to model the categorical variables as fixed instead of random effects in a linear regression model \parencite{borenstein2021introduction, cooper2019handbook, nakagawa2023quantitative}. We use a linear regression model to analyze the impact of the variables on the effect sizes. With the results of the regression model, we can identify influential categories and variables and analyze their influence on the bias detections, focusing on biases and models. We regress the experiment variables on both the original \textit{bias detected} and \textit{bias detected (capped)} targets but focus on the capped target variable to not distort the regression results with extreme effect sizes. The regressions are modeled as follows (exemplary for capped target):
\begin{equation} \label{eq:general_regression_capped}
    \text{\textit{bias detected (capped)}} = \beta_0 + \beta_1 \text{\textit{bias}} + \beta_2 \text{\textit{model}} + \beta_3 \text{\textit{scenario}} + \beta_4 \text{\textit{temperature}} + \epsilon
\end{equation}


\subsubsection{Scenario impacts on bias detections}
\par We also aim to assess the impact of the scenarios on the bias detections in the models. We are especially interested in whether the removal of the persona and the odd and large values have any effect on the biasness. Primarily, we use the previously presented regression models of all experiment variables including the scenarios on the bias detected variables. By analyzing the regression coefficients, we can identify the impact of the scenarios compared to the base scenario. We further regress only the scenarios on the bias detections in order to analyze the explained variance of the scenarios. We expect that the inclusion of a persona as well as normal values trigger more biased responses in the models.

\subsubsection{Explainability of biases through model features}
\par To further analyze the bias occurrences in the LLMs, we investigate different model features and whether they can explain the degree of biasness. We collect data on the models' release, update and knowledge cutoff dates, their parameters and context length as well as some performance metrics (MMLU and Chatbot Arena). With the time of release and update, we aim to analyze whether newer models possess different bias characteristics than older models. The knowledge cutoff refers to the latest date training data was included. Further, we expect larger language models i.e. with more parameters to ingest more non-linearities and biases than smaller models. The context length is also a major
 factor in the models' reasoning capabilities and thus could influence the bias detections \parencite{naveed2023comprehensive, zhao2023survey}.

\par MMLU (Massive Multitask Language Understanding) is a benchmark for evaluating knowledge capabilities across various domains. The test covers 57 tasks with different difficulties and can also offer insights into a model's reasoning abilities. It is widely adopted and included in model announcements most of the time \parencite{hendrycks2020measuring}. We also include the scores of the Chatbot Arena which enables users to choose a preferred option of paired responses to rank the models. The Chatbot Arena is widely used (187 models with nearly 2.5 million votes) and provides insights into a model's human likeness and alignment which could hint at more or less biased models \parencite{chiang2024chatbot, huggingface2024chatbotarena}.

\par It should be noted that while open-source models are well documented, we also had to estimate some model features for closed-source models. The models and their features are detailed in Appendix \ref{appendix:models}. As in the previous analysis, we regress the model features and the specified model temperature in the experiment on the bias detections and analyze their explainability towards and impact on the bias detections.