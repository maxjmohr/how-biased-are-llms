\section{Study design and methodology}
\par This section provides an overview of the methodology used in this research. We outline the selected biases and models which form the scope of the study. Further, we outline the structural design of the studies as well as the prompts and various scenarios to be tested. To evaluate the models' behaviors, we present an approach to score the bias detections and based on these results, analyze the impacts of different scenarios and models.


\subsection{Bias selections}
\par As described, previous research revolving around cognitive biases has shown that there are numerous biases influencing human behavior. Thus, we have to sample a concise yet ideally comprehensive sample of biases to explore more generalizable analysis results. To narrow down the range of biases, we focus on biases affecting (economic) decision-making processes. In particular, we aim to recreate and perhaps slightly modify existing experiments.

\setlength{\parindent}{20pt}
\par \textbf{Anchoring bias} The bias known as anchoring is often used to explain why people tend to rely heavily on an initial piece of information ("anchor") in their decision-making afterwords. We recreate the experiment from \textcite{tversky1974judgment} where participants are first asked whether the portion of African countries in the United Nations is higher or lower than a certain number and afterwards estimate the exact percentage. Their results showed that the initial percentage had a significant effect on the exact estimation afterwards.

\par \textbf{Category size bias} Decision-making is also influenced by the way alternatives are categorized and distributed, a phenomenon known as the category size bias \parencite{isaac2014judging}. For example, an investor's expectation about the performance of a particular stock could be influenced by the number of other stocks in the portfolio that belong to the same industry. Similarly, \textcite{tversky1994support} showed that participants judged the probability of dying of unnatural cases differently when natural causes were presented as one category or  broken down into individual categories. To test for the category size bias, we replicate an experiment from \textcite{isaac2014judging} where participants estimate the probability of randomly selecting a ball from a lottery, itself containing balls with three different colors and varying category sizes. The output of the experiment showed that if the ball is drawn from a larger category, the probability is estimated higher even though the actual probability is equal.

\par \textbf{Endowment effect} The human tendency to value objects in their possession (endowment) higher than if they did not own them is known as the endowment effect. This effect is strongly linked to loss aversion, where people demand more to give up an item than they would be willing to pay to acquire it \parencite{kahneman1990experimental}. Research has also shown that the effect is independent of whether sellers actually sell the product or exchange similarly valued goods \parencite{knetsch1989endowment}. \textcite{kahneman1990experimental} demonstrated the effect by asking participants for their willingness to pay for a mug versus their willingness to accept compensation for the mug. The results showed that participants owning the mug valued it at more than double the value than the other participants.

\par \textbf{Framing effect} Framing is a cognitive bias where humans react significantly differently to an equivalent choice depending on how it is presented. The effect is strongly influenced by the loss aversion of humans as well as them heavily relying on their emotional state \parencite{tversky1981framing}. To detach the experiment on framing effect as well as possible from the experiment on loss aversion, we target to simulate two scenarios that both have losses (not comparison between loss and gain framing). We recreate the experiment by \textcite{tversky1981framing} where participants were asked to buy another concert ticket after having lost either the original ticket or the same amount of money (different framing of lost monetary value). The study showed that participants were more likely to buy the ticket when they lost the money as from the human perspective, the lost money is decoupled from the ticket purchase.

\par \textbf{Gambler's fallacy} The gambler's fallacy is the mistaken belief that the probability of a certain event occurring is influenced by the frequency of prior events, despite each event being independent \parencite{bar1991perception,kovic2019gambler}. Known as an "insensitivity to sample size", humans also often tailor their decisions to a small sample size which, in their mind, represents the distribution of the larger sample. For instance, humans might quickly adapt their judgement based on the law of large numbers even though they are faced with random events \parencite{tversky1974judgment}. The shifts in probability perceptions can thus be major causes for biased (economic) decision-making. While there has been research on the mechanisms influencing the fallacy (for example the presentation of information \parencite{barron2010role}) as well as possible side effects and fallacies \parencite{kovic2019gambler}, we focus on assessing whether the models are prone to the fallacy through an experiment involving coin flips (similar to the Monte Carlo fallacy).

\par \textbf{Loss aversion} Loss aversion describes the human tendency to prefer avoiding losses over acquiring gains of the same value  \parencite{liu2023review}. Within their research on prospect theory (decision-making biases under risk and uncertainty), \textcite{tversky1992advances} estimated that losses are twice as impactful as gains. Closely related to the framing effect, the presentation of the same base information in more risk-averse and risk-seeking scenarios can have significant impact on human decision-making \parencite{druckman2001evaluating}. This bias is particularly relevant in the context of economic decision-making and has been applied to various fields such as retail sale strategies (discount for additional spending), financial investments (sell winners, hold losers) and more \parencite{liu2023review}. We recreate the experiment from \textcite{thaler2015misbehaving} which phrases two scenarios as a financial loss and a gain to test for loss aversion. The results showed that participants were more likely to take risks in the loss scenario than in the gain scenario as loss aversion was a stronger motivator than risk aversion.

\par \textbf{Sunk cost fallacy} This effect refers to the human phenomenon to preferring an option due to a prior investment into it (sunk costs) even though a better alternative would be available \parencite{arkes1985psychology}. Due to this, even temporally distant investment decisions can have a substantial impact on the decision-making process. The sunk cost fallacy is also strongly intertwined with other cognitive biases, most notably loss aversion or commitment bias \parencite{jarmolowicz2016sunk}. We choose to examine the experiment introduced by \textcite{arkes1985psychology} where participants are asked to decide between two ski trip scenarios with different sunk costs. In their study, the participants were more likely to choose the more expensive trip (but being the less attractive trip) even though the costs were already sunk.

\par \textbf{Transaction utility theory} Transaction utility describes perceived psychological non-monetary gains of a deal that are beyond the actual economic value. \textcite{thaler1983transaction} defined the concept as the difference between the actual price and one's reference price. A simple example is the satisfaction from buying a product itself; the additional joy of saving 20 percent is the transactional utility in the purchase. To test this theory, we will recreate an experiment where participants have the option to buy two products either directly or at a store 20 minutes away, each for 20 percent discount. The difference in the scenarios is that one product is cheap (radio) and the other is expensive (television) \parencite{thaler1983transaction}. The results showed that participants were more likely to buy the cheaper product directly without the discount even though the absolute savings were the same.

\setlength{\parindent}{0pt}
\par With the selected biases, we aim to examine the cognitive decision behavior of large language models as broadly as possible. The exact replications and wordings of the experiments are described in the appendix.


\subsection{Model selections}
\par The development and publication of new models and model architectures is ongoing and rapid. We focus on models which are widely available to the public and have seen significant use in research and industry. The models we selected are those of major technology companies and have some similarities but also differences in their architecture, size and training data. We aim to include models from different companies to ensure a broad analysis of the models' behavior. The models we selected are:

\setlength{\parindent}{20pt}
\par \textbf{Gemma2 (Google\footnote{Google, founded in 1998, is a global technology company specializing in internet-related services and products, including search engines, online advertising, and AI; in 2023, its parent company Alphabet reported a revenue of over \$305 billion \parencite{alphabet2023annual}.})} The Gemma model family is Google's offering of open language models. The latest and second generation Gemma models were released in late June 2024. They exhibit a decoder-only transformer architecture for text-to-text tasks and are designed similarly to Google's larger and private models (\textit{Gemini}). The focus of the Gemma2 models lies on the availability and usability to the public and thus exist in two, seven and 25 billion parameter sizes \parencite{team2024gemma}. For our research, we examine the reasoning capabilities and existence of biases in the seven and 27 billion parameter versions.

\par \textbf{GPT-4o (OpenAI\footnote{OpenAI, founded in 2015 as a non-profit but now a capped for-profit, is a leading AI research and deployment company. OpenAI's primary products include the GPT series of language models as well as text-to-image and text-to-vision models \parencite{openai2024about}.})} With the introduction and rash success of OpenAI's artificial intelligence chatbot \textit{ChatGPT} in 2022, the models backing the chatbot (GPTs) have a significant impact on the field of natural language processing and in communicating the latest advancements in AI to the public. The latest iteration are the GPT-4o models (\textit{o} standing for "omni" or multi-modal) which are designed to be more human-like and capable of reasoning and understanding context. The architecture of GPT-4o is similar to its predecessors, a transformer model which however includes additional layers for multimodal input, with encoders for images, videos and audio. While language processing and reasoning has improved marginally compared to GPT-4 Turbo, the tokenizer has become much more efficient and thus the model is capable of processing more data in less time \parencite{achiam2023gpt, openai2024gpt4o}. We analyze GPT-4o mini, which is natively used for the free \textit{ChatGPT} service and despite its smaller parameter size outperforms the previously established model GPT-3.5 Turbo, and GPT-4o, which is the largest model and available to premium users \parencite{openai2024gpt4omini}.

\par \textbf{Llama3.1 (Meta\footnote{Meta, formerly known as Facebook, is a multinational technology conglomerate founded in 2004. It rebranded as Meta in 2021 to reflect its expansion into the metaverse and artificial intelligence. In 2023, Meta reported a revenue of \$134 billion \parencite{meta2024about}.})} Llama3.1 is Meta's latest iteration of their open-source language model family, following the success of Llama2 and 3. Meta's offering of these foundational models has significant impact in building and finetuning personalized multilingual solutions and for any research purpose. Llama 3.1 models come with much longer context length and more as well as higher quality training data than previous models while maintaining the similar transformer architecture of previous iterations. While we will analyze the eight and 70 billion parameter models, a 405 billion parameter version is also available and rivals the largest closed-source models \parencite{dubey2024llama,meta2024llama31}.

\par \textbf{Phi3.5 (Microsoft\footnote{Microsoft, founded in 1975, is a global technology company known for its software products (revenue of \$211 billion). With recent large investments such as in OpenAI, Microsoft plays a significant role in the AI space \parencite{microsoft2024about}.})} The Phi models by Microsoft are considered SLMs (small language models) and focus on efficiency and speed. While this does lead to less factual knowledge in the models, they are still competitive regarding reasoning and coding tasks even when implemented and running on phones. There are multiple sizes and different architectures of the Phi models to account for multilingual or multimodal inputs. In particular, Phi 3.5 has a much expanded context length limit from 4 to 128 thousand tokens. We analyze the new Phi 3.5 Mini model (roughly 4 billion parameters) and the slightly larger Phi 3 Medium model (14 billion parameters). With these smaller models we hope to cover not only the spectrum of large language models but also small ones \parencite{abdin2024phi}.

\setlength{\parindent}{0pt}
\par In total, we analyze four model families and eight distinct models. Besides the GPT models of OpenAI, the models are run locally. These models are downloaded as well as accessed through Ollama, a wrapper around llama.cpp which allows for efficient inference of most open-source models \parencite{gerganov2023llamacpp}. To minimize discrepancies in prompting and response extraction between the models, we use the language model framework LlamaIndex to interact with the models and build a unified experiment pipeline \parencite{liullamaindex2022}. More details on the models and their configurations are provided in the appendix.


\subsection{Structuring of experiments}
\par The following will depict the construction of the experiments. For the term \textit{experiment}, we refer to a unique combination of a bias with a scenario (as detailed in section \ref{methodologies:scenarios}) run on a model with a certain temperature.

\subsubsection{Study designs}
\par We target to structure all experiments in a consistent manner to ensure comparability and a unified analysis afterwards. Most studies which we base our experiments on are structured in a way that two or more questions with slightly different wording are asked to the participants. Some studies ask for value estimations while others are constructed as choice experiments (two choice options i.e. A and B). The response distributions to these questions are then compared to detect biases, identically or similarly to control and treatment group study designs. We adopt this design for each experiment where we prompt the model two questions with slight variations in wording 100 times each to form a control and treatment group \parencite{cohen1988statistical,morris2002combining}. In case we cannot correctly extract a model response, we reiterate the question as often as necessary to receive 100 valid responses. One exception is the study regarding the sunk cost fallacy where participants are asked only once to choose between two scenarios with different sunk costs \parencite{arkes1985psychology}. To align with our study design, we add a second question to act as the control where the sunk costs are identical and thus expect different and unilateral response distribution (as there is now no economical and solely a subjective reason to pursue the choice the participants feel like doing).

\par Studies comparing two targets can be constructed as two-group design studies with independent groups or a one-group repeated measure \parencite{dunlap1996meta,goulet2018review}. Generally, to determine whether an experimental design should be classified as a two-group independent design or a single-group repeated measures design, several key assumptions must be considered. In an independent groups design, the fundamental assumption is that each group is treated as independent, meaning no participant or entity contributes data to more than one group, and there is no connection between the observations in the two groups \parencite{morris2002combining}. This design is typically used when participants or systems are randomly assigned to different conditions or when different instances are treated independently \parencite{cohen1988statistical,morris2002combining}. In contrast, a repeated measures design assumes that the same participants or system are tested under multiple conditions. This design captures within-subject variation, meaning each entity's performance under one condition is directly related to its performance under the other conditions. In this case, it is necessary to account for within-subject correlations because the same participant or system retains information or characteristics across conditions \parencite{dunlap1996meta,morris2002combining}.

\par Given our experimental design and technical implementation, each model is re-initialized between each question, meaning that it does not retain any memory or internal state between trials. Although the same model architecture is used, the fact that it is re-initialized before running the next question ensures that the responses from both groups are independent. There is no inherent correlation between the models' performances in group A and group B due to this re-initialization, which mimics the behavior of having entirely independent models for each group. Therefore, the key assumption of independence of observations is met, aligning this study with an independent two-group design. In a true repeated measures design, the models would have needed to answer both sets of questions (A and B) without resetting their state, creating a paired structure where each observation in group A could be directly compared with its counterpart in group B. Since we did not implement a paired output of both questions which would force an investigation and integration of their correlation, we argue that the appropriate framework for analysis is that of two independent groups \parencite{dunlap1996meta}.

\subsubsection{Robust base prompt template}
\par For all experiments, we target a basic prompting style similar to the questionnaires in the original studies to maintain consistency and control over the interactions across the models. Research suggests that well-structured prompts can significantly impact the quality and reliability of model outputs, especially in complex tasks like bias detection and structured information retrieval \parencite{chen2023unleashing,santu2023teler}. \textcite{santu2023teler} present a prompt taxonomy TELeR (Turn, Expression, Level of Details, Role) with suggestions on how to structure prompts for different, especially complex tasks. They generally suggest for higher complexity, the level of details and focus on structure in the prompt should increase. As we try to avoid too much additional details other than the survey itself, we orientate ourselves towards their "multi-sentence, paragraph-style" directive.

\par \textcite{sahoo2024systematic} divide prompt engineering techniques by their application domains such as reasoning tasks, reducing hallucination, generating code and many more. They present many advanced chaining and multi-step prompt engineering techniques but refer to simple prompting techniques (zero- and few-shot prompting) for new tasks without extensive model training. Especially in zero-shot settings, models rely heavily on their general knowledge gained during training. This aligns with the study designs on human participants which in most cases, are not experts and do not receive additional information besides the survey questions. Concise and clear explanations improve the response quality and in some scenarios with basic tasks, zero-shot prompts can significantly outperform guided, few-shot prompts \parencite{chen2023unleashing,ekin2023prompt}. In zero-shot scenarios and likewise in our experiments, the prompt aims to trigger a model response which it was previously not trained on \parencite{marvin2023prompt}.

\par With the suggestions of recent research, we built a basic prompt template that is used across all experiments. We do not provide additional context to the study questions besides some output formats suggestions for structured predictions (answering with a single letter or numerical value) \parencite{schmidt2024towards}. In this regard, the providing the hint that the models were only initialized to generate two tokens, improved the response quality. After some refinements and iterative testing to receive mostly structured and usable responses, we settled on the following prompt template:
\begin{table}[ht]
    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{System message}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    --------------------------
    \textit{Persona}
    --------------------------\\
    You will be asked to make choices. Please blank out that some information might be missing or that you might not be able to make a choice. I have initialized you in a way that you can only generate 2 tokens. The only valid answer is \textbf{a single letter or number}.
    \end{tcolorbox}

    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{User message}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    You are forced to choose! Answer the experiment by only giving the letter of the answer options (e.g. A, B, C, ...) or a numerical value (80, 100, 1000, ...). Do not state anything else! Do not hallucinate.\\
    --------------------------
    \textit{Study question}
    --------------------------\\
    Your output should only be a LETTER (A, B, C, ...).
    \end{tcolorbox}

    \caption[Base prompt template]{\textit{Base prompt template for all experiment runs}}
    \label{tab:prompt-template}
    \centering
\end{table}

\par Another significant aspect of prompt engineering revolves around the addition of personas to the prompts. Personas are fictional characters that represent the target audience of the model and can be used to guide the model's responses towards a specific direction. Consisting of various persona variables, which can reflect demographics and other characteristics, personas have shown to influence large language models in their tasks and responses \parencite{chen2023unleashing,hu2024quantifying,olea2024evaluating}. \textcite{hu2024quantifying} depict that personas can have small but significant impacts on the models' outputs. Their research suggests that models are able to adapt their choices to the given persona but only in a narrow range. They find that the more persona variables are directly correlated with the response variable of the model, the higher the impact of the persona. \textcite{olea2024evaluating} compare single-agent personas (either fixed or fitted to the task) and multi-agent personas and find that for simple tasks with short answers, single-agent personas are more effective. While simple personas show slight gains, especially the expert personas, which were auto-generated by a LLM for each experiment question, show significant improvements in the models' responses.

Like the studies we aim to recreate, we strive to measure a bias effect across an average, non-specific audience. For that reason and to translate the findings into our study, we create a simple persona with some general characteristics to fit with each experiment and aligning it to examples from \textcite{brand2023using}. This prohibits us from adding specific and correlated characteristics (relating to an expert persona) per experiment but should still provide a general guidance for the models' responses:
\begin{table}[ht]
    \begin{tcolorbox}[
        colframe=gray!90,    % Border color
        colback=white,     % Background color
        boxrule=0.4mm,     % Border thickness
        arc=5mm,           % Rounded corners
        title=\textbf{Persona}, % Title of the box
        fonttitle=\bfseries,   % Bold title
    ]
    You are a customer with median income and average education. You are selected at random to participate in a survey. You can only choose one of the presented options or assign a value. Behave humanlike and choose instinctively. You can and are advised to make a subjective decision! There is no right or wrong, but you \textbf{have to decide}.
    \end{tcolorbox}

    \caption[Persona description]{\textit{Persona description across all experiments}}
    \label{tab:persona-description}
    \centering
\end{table}

\subsubsection{Scenarios in form of prompt adjustments}
\label{methodologies:scenarios}
\par We have set up the base prompt format and biases and models to recreate the studies. This enables us to test for the existence of cognitive biases in LLMs. While the study design can reflect the attained behavior of a model, the responses could simultaneously be influenced by the studies being part of the training data \parencite{brand2023using}. To analyze the reliance of the detected biases on familiar and learned studies, we create different scenarios across all experiments. The scenarios can range from structural prompt changes to changes of variable values in the experiments. Among finetuning of LLMs, \textcite{brand2023using} modify persona descriptions and add market insights to examine the model's product preferences. Similarly, \textcite{kojima2022large} extend the prompts by a single instruction to improve the reasoning capabilities of the models. \textcite{mizrahi2024state} observe the sensitivity of LLMs towards the exact paraphrasing of an instruction. With our scenarios, we mainly aim to test whether the biases change with different instructions and unseen or odd and unrealistic data, as shown in Table \ref{tab:scenarios}.
\begin{table}[h!]
    \centering
    \begin{tabular}{@{}p{4cm}p{11cm}@{}}
    \toprule
    \textbf{Scenario} & \textbf{Description} \\ \midrule
    Base & Replication of the original study using base prompt format \\ \addlinespace
    Without Persona & Base scenario but exclusion of persona \\ \addlinespace
    Odd values & Base scenario but multiplication of the study variables (e.g. prices) times 9.7 \\ \addlinespace
    Large values & Base scenario but multiplication of the study variables (e.g. prices) times 55,555.5 \\ \bottomrule
    \end{tabular}
    \caption[Scenario overview]{\centering \textit{Overview of scenarios across experiments and their descriptions on prompt adjustments}}
    \label{tab:scenarios}
\end{table}


\subsection{Response analysis}

\subsubsection{Processing responses into a bias indicator}
\label{methodologies:biasdetector}
\par With our study design of a control and test group per experiment, we aim to compare the response distributions to measure bias effects. While we are generally concerned with the existence of a bias in an experiment configuration, we especially aim to quantify the magnitude of the bias for more detailed comparisons and analysis between the experiments. With respect to identifying a bias effect, one could implement hypothesis tests for determining whether the response distributions differ significantly between the control and treatment groups. However, \textcite{sullivan2012using} suggest that while P values may indicate a significant effect, the size of the effect is not extractable. P values can be influenced by the effect size of differences or sample sizes and can thus be difficult to interpret. Significant P values do not necessarily stem from large effect sizes but can also e.g. come from a large sample size and small (and practically irrelevant) effects \parencite{borenstein2021introduction,sullivan2012using}. Further, even though methodology does exist to comparing P values and incorporating them into meta analyses, strong assumptions are needed to make the results comparable \parencite{borenstein2021introduction}.

\par As the comparability of the experiments is a key aspect of our study and all experiments have similar structures and examine the same topic that is cognitive biases in large language models, we orient ourselves towards traditional meta analysis methodologies by computing a determined effect size per experiment. In our setting of two independent groups (pre- and post-test measurements), we can calculate the standardized difference of means (Cohen's d) as \parencite{borenstein2021introduction,goulet2018review,morris2002combining}:
\begin{equation} \label{eq:cohensd}
d = \frac{\bar{X}_{post} - \bar{X}_{pre}}{SD_{pooled}}
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $\bar{X}_{pre}$, $\bar{X}_{post}$:& means of the control and treatment groups, respectively \\
    $SD_{pooled}$:& pooled standard deviation of both groups
\end{tabular} \\

\par The pooled standard deviation is correspondingly calculated as:
\begin{equation} \label{eq:cohensd_sd}
  SD_{pooled} = \sqrt{\frac{(n_{pre} - 1) \cdot SD_{pre}^2 + (n_{post} - 1) \cdot SD_{post}^2}{n_{pre} + n_{post} - 2}}
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $n_{pre}$, $n_{post}$:& sample sizes of the control and treatment groups \\
    $SD_{pre}$, $SD_{post}$:& standard deviations of the control and treatment groups
\end{tabular} \\

\par The effect size $d$ is a standardized measure and thus allows for comparisons between different experiments and studies. Cohen's $d$ can be interpreted as the magnitude of the bias effect, with larger values indicating a stronger bias effect. The effect size can be further interpreted using the guidelines by \textcite{cohen1988statistical} where values of 0.2, 0.5 and 0.8 are considered small, medium and large effects, respectively. While Cohen's $d$ is not an unbiased estimator of the population effect size due to overestimation and a correction factor has been developed to form Hedges' $g$ \parencite{hedges1981distribution}, \textcite{goulet2018review} suggest that the correction factor is especially necessary for small sample sizes $n_{total} < 16$. Therefore, we use the uncorrected Cohen's $d$ for our analysis as all sample sizes in our experiments are $n_{total} = 200$.

\par For our experiments where we ask models to respond with numerical values, we can directly calculate the means and standard deviations of the responses to compute the effect size. To align with these experiments, we convert the models' responses in the choice experiments (i.e. A and B) to binary values, depending on which option we expect the models to choose in the treatment group. In the case of \textit{loss aversion}, we expect the control group to select the risk averse gain option ($A=0$) and the treatment group to select the risk seeking loss option ($B=1$) \parencite{thaler2015misbehaving}. We then calculate the effect size based on the binary responses.

\par By nature of Cohen's $d$ calculation (Equation \ref{eq:cohensd}), the effect size can also be negative if the control group has a higher mean than the treatment group. For most of our bias experiments, we have recreated the experiments in a way that a higher mean in the treatment group refers to the expected bias effect. We thus expect a positive effect size for the existence of a bias and interpret any negative or zero effect sizes as no bias detected. Exceptions to this are the experiments regarding the \textit{framing effect} and \textit{transaction utility bias}. For these biases, a differentiating effect in either group hints at the existence of the bias. Therefore, we adjust the negative effect sizes by extracting their absolute values. The resulting metric, namely \textit{bias detected}, is the following:
\begin{equation} \label{eq:bias_detected}
    \text{\textit{bias detected}} =
    \begin{cases}
        |d| & \text{if bias = \textit{framing effect} or \textit{transaction utility}} \\
        d & \text{else}
    \end{cases}
\end{equation}
\hspace{0.5cm} \textit{where:} \\
\hspace*{3em}
\begin{tabular}{rl}
    $d$:& Cohen's $d$ effect size by Equation \ref{eq:cohensd}
\end{tabular}

\subsubsection{Analysis of effect sizes across biases and models}
\label{methodologies:analysisbiasmodels}
\par The \textit{bias detected} metric allows us to compare the magnitude of the bias effects across all experiments. To follow our research question of whether biases are present in large language models and how they differ between biases and models, we group all experiment detections by bias and model. \textcite{morris2002combining} suggest an average weighted by the reciprocal of the sampling variance of the effect sizes to aggregate effect sizes. Per experiment, the sampling variance is influenced by the sample sizes and the population effect size (unweighted average). As all our experiments have the identical sample sizes and each group has one fixed unweighted population effect size, the sampling variance is constant across all experiments. We can thus directly average the effect sizes to aggregate the bias detections. With the aggregated effect sizes, we compare the detections across biases and models to analyze the presence and magnitude of biases in the models.

\par To simplify and illustrate the aggregated bias detections, we cap the aggregates at zero and one. Group effect sizes larger than one are set to one to indicate a large bias effect, while those smaller than zero are set to zero to indicate no detected biases. 
As we solely cut off the aggregates and do not rescale the effect sizes themselves, we ensure to correctly weigh extremely large and small effect sizes in the aggregation. Further, by capping the aggregates and not rescaling them, we maintain the interpretation guidelines by \textcite{cohen1988statistical} where values of 0.2, 0.5 and 0.8 are considered small, medium and large effects, respectively.

\par Besides the bias detections themselves, we test for homogeneity of the effect sizes across the models and biases. 

\par Then look at homogeneity, again with grid maybe. And then some high- and lowlights. Also in the end state (in the analysis itself) that this is the reason why we must do further analysis of the impacts and models, to see what other factors might influence the bias detection.



\subsubsection{Scenario impacts on bias detections}
Average treatment effects of treated on randomized values as well as explicitly prompting humanlike behavior. The control group is the normal prompt.
Perhaps with Repeated Measures ANOVA (RM-ANOVA) to see if there are significant differences between the scenarios.
Perhaps Mixed-Effects Models to see if there are significant differences between the scenarios.
Friedman test? https://datatab.de/tutorial/friedman-test

\subsubsection{Explainability of biases through model features}
Are there any trends between newer/larger models?
Temperature? Ggf nur für das normale Szenario da es sonst die Runs sprengt

Train XGBoost model, get feature importances and see if there are important features that can explain whether a model is more biased or less.
